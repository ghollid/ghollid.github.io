---
title: "Favorite Machine Learning Method"
author: "Grace Holliday"
date: "2023-07-11"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gbm)
library(rmarkdown)
library(caret)
## render("C:/Users/Sarah/Documents/ghollid.github.io/_Rmd/2023-07-11-Favorite-Machine-Learning-Method.Rmd",output_format="github_document",output_dir="../_posts/",output_options=list(keep_html=FALSE))
```

# The Most Interesting Machine Learning Method

I think the most interesting method that I have learned in this class is 
boosting.  Boosting is a general approach that can be applied to trees and 
allows for the slow training of trees. The trees are grown sequentially with 
each subsequent tree being grown on a modified version of the original data. 
The predictions are then updated as the trees are grown, and new trees are 
grown by considering the errors in the trees previously created. Lambda 
represents a shrinkage parameter than slows the fitting process.

# Fitting a Boosted Model
```{r, echo=TRUE}
# First setting seed for reproducibility.
set.seed(1234)

# Divide data into training and test set.
train <- sample(1:nrow(iris), size = nrow(iris)*.70)
test <- setdiff(1:nrow(iris), train)

# training and testing subsets
datTrain <- iris[train, ]
datTest <- iris[test, ]
```

```{r, results="hide"}
## fitting amodel
boostFit <- train(Petal.Length ~ I(Sepal.Length^2)+Species+Sepal.Width,
                data=iris, method='gbm',
                preProcess=c("center","scale"),
                trControl=trainControl(method='cv',number=5),
                tuneGrid=expand.grid(n.trees=seq(25,200,50),
                                     interaction.depth=seq(1,4,1),
                                     shrinkage=0.1, n.minobsinnode=10))
```

```{r, echo=TRUE}
plot(boostFit)

boostFit$finalModel

#Run on test data
boostFit.predict <- predict(boostFit, newdata = datTest)

#Obtain RMSE from test set
boostFit.RMSE <- postResample(boostFit.predict, obs = datTest$Petal.Length)
boostFit.RMSE
```
This model has a relatively high R^2^ value, but the RMSE probably isn't as low 
as we would like.